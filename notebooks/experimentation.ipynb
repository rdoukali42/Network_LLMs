{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2db128e",
   "metadata": {},
   "source": [
    "## Quick System Test\n",
    "\n",
    "Test basic functionality without complex configuration dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e713d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test of basic components without config dependencies\n",
    "print(\"üöÄ Testing Basic Components...\")\n",
    "\n",
    "# Test 1: Calculator tool (no config needed)\n",
    "try:\n",
    "    from tools.custom_tools import CalculatorTool\n",
    "    calc_tool = CalculatorTool()\n",
    "    result = calc_tool._run(\"10 * 5\")\n",
    "    print(f\"‚úÖ Calculator: 10 * 5 = {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Calculator test failed: {e}\")\n",
    "\n",
    "# Test 2: Environment variables\n",
    "try:\n",
    "    import os\n",
    "    google_key = os.getenv('GOOGLE_API_KEY')\n",
    "    langfuse_key = os.getenv('LANGFUSE_PUBLIC_KEY')\n",
    "    print(f\"‚úÖ Google API Key: {'Configured' if google_key else 'Missing'}\")\n",
    "    print(f\"‚úÖ LangFuse Key: {'Configured' if langfuse_key else 'Missing'}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Environment test failed: {e}\")\n",
    "\n",
    "# Test 3: Basic imports\n",
    "try:\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    print(\"‚úÖ Google Generative AI imports working\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Google AI import failed: {e}\")\n",
    "\n",
    "print(\"\\nüéâ Basic tests completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f65a55",
   "metadata": {},
   "source": [
    "# AI System Experimentation Notebook\n",
    "\n",
    "This notebook provides an interactive environment for experimenting with the AI system components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4985c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(project_root / \".env\")\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(\"Environment loaded ‚úÖ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653a7649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project components\n",
    "from pathlib import Path\n",
    "from config.config_loader import ConfigLoader\n",
    "from utils.helpers import load_sample_data\n",
    "from vectorstore.vector_manager import VectorStoreManager\n",
    "from agents.base_agent import ResearchAgent, AnalysisAgent\n",
    "from evaluation.llm_evaluator import LLMEvaluator\n",
    "\n",
    "print(\"Components imported successfully ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d425d0",
   "metadata": {},
   "source": [
    "## Configuration Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd77f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from config.config_loader import ConfigLoader\n",
    "\n",
    "# Check if config files exist first\n",
    "project_root = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "config_dir = project_root / \"configs\"\n",
    "\n",
    "print(f\"Config directory: {config_dir}\")\n",
    "if config_dir.exists():\n",
    "    config_files = [f.name for f in config_dir.iterdir() if f.suffix == '.yaml']\n",
    "    print(f\"Available config files: {config_files}\")\n",
    "    \n",
    "    if 'development.yaml' in config_files:\n",
    "        # Create config loader with correct path\n",
    "        local_config_loader = ConfigLoader(str(config_dir))\n",
    "        config = local_config_loader.load_config(\"development\")\n",
    "        print(\"\\nDevelopment Configuration:\")\n",
    "        print(f\"LLM Model: {config['llm']['model']}\")\n",
    "        print(f\"Temperature: {config['llm']['temperature']}\")\n",
    "        print(f\"Chunk Size: {config['chunking']['chunk_size']}\")\n",
    "        print(f\"Vector Store: {config['vectorstore']['type']}\")\n",
    "    else:\n",
    "        print(\"‚ùå development.yaml not found\")\n",
    "else:\n",
    "    print(\"‚ùå Config directory not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0045e1f7-1894-45b1-9cf2-6425ac888b89",
   "metadata": {},
   "source": [
    "## Sample Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e679aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine sample data\n",
    "sample_docs = load_sample_data()\n",
    "\n",
    "print(f\"Loaded {len(sample_docs)} sample documents\")\n",
    "print(\"\\nFirst document preview:\")\n",
    "print(sample_docs[0][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e130b42-3172-4f69-9e73-d219faca977e",
   "metadata": {},
   "source": [
    "## REAL Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe23242c-e7a4-487b-870f-c42e6c765d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine sample data\n",
    "# sample_docs = load_Company_data()\n",
    "\n",
    "# print(f\"Loaded {len(sample_docs)} sample documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70003c6c",
   "metadata": {},
   "source": [
    "## Vector Store Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d816bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vector store (requires API keys)\n",
    "try:\n",
    "    vector_manager = VectorStoreManager(config)\n",
    "    print(\"Vector store initialized ‚úÖ\")\n",
    "    \n",
    "    # Add sample documents\n",
    "    vector_manager.add_documents(sample_docs)\n",
    "    print(\"Sample documents added to vector store ‚úÖ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Vector store initialization failed: {e}\")\n",
    "    print(\"This is expected if API keys are not configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6f1065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test similarity search\n",
    "try:\n",
    "    query = \"What is machine learning?\"\n",
    "    results = vector_manager.similarity_search(query, k=2)\n",
    "    \n",
    "    print(f\"Search results for: '{query}'\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\nResult {i}:\")\n",
    "        print(f\"Content: {result['content'][:150]}...\")\n",
    "        print(f\"Score: {result['score']:.4f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Search failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70edf93",
   "metadata": {},
   "source": [
    "## Agent Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0a949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agents\n",
    "research_agent = ResearchAgent()\n",
    "analysis_agent = AnalysisAgent()\n",
    "\n",
    "print(\"Agents initialized:\")\n",
    "print(f\"- {research_agent.name}\")\n",
    "print(f\"- {analysis_agent.name}\")\n",
    "\n",
    "print(\"\\nResearch Agent System Prompt:\")\n",
    "print(research_agent.get_system_prompt())\n",
    "print(\"\\nAnalysis Agent System Prompt:\")\n",
    "print(analysis_agent.get_system_prompt())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cce2244",
   "metadata": {},
   "source": [
    "## Evaluation Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f11796b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test evaluation system\n",
    "try:\n",
    "    evaluator = LLMEvaluator(config)\n",
    "    print(\"Evaluator initialized ‚úÖ\")\n",
    "    # print(\"\\nEvaluator Agent System Prompt:\")\n",
    "    # print(evaluator.get_system_prompt())\n",
    "    \n",
    "    # Test evaluation parsing\n",
    "    test_eval = '{\"score\": 8, \"explanation\": \"Good response with relevant information\"}'\n",
    "    parsed = evaluator._parse_evaluation(test_eval)\n",
    "    print(f\"\\nParsed evaluation: {parsed}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Evaluator initialization failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f7001c",
   "metadata": {},
   "source": [
    "## Experiment Configuration Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1830e0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get correct config path\n",
    "project_root = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "local_config_loader = ConfigLoader(str(project_root / \"configs\"))\n",
    "\n",
    "# Load experiment configurations\n",
    "exp1_config = local_config_loader.load_experiment_config(\"experiment_1\")\n",
    "exp2_config = local_config_loader.load_experiment_config(\"experiment_2\")\n",
    "\n",
    "print(\"Experiment 1 Configuration:\")\n",
    "print(f\"Temperature: {exp1_config['llm']['temperature']}\")\n",
    "print(f\"Chunk Size: {exp1_config['chunking']['chunk_size']}\")\n",
    "print(f\"Max Iterations: {exp1_config['agents']['max_iterations']}\")\n",
    "\n",
    "print(\"\\nExperiment 2 Configuration:\")\n",
    "print(f\"Temperature: {exp2_config['llm']['temperature']}\")\n",
    "print(f\"Chunk Size: {exp2_config['chunking']['chunk_size']}\")\n",
    "print(f\"Max Iterations: {exp2_config['agents']['max_iterations']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b9d35c",
   "metadata": {},
   "source": [
    "## End-to-End System Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992631c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple direct test - calculator tool\n",
    "try:\n",
    "    from tools.custom_tools import CalculatorTool\n",
    "    \n",
    "    calc_tool = CalculatorTool()\n",
    "    result = calc_tool._run(\"14 + (-6)\")\n",
    "    print(f\"Calculator test: 14 - 6 = {result}\")\n",
    "    print(\"Basic tool test successful ‚úÖ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Tool test failed: {e}\")\n",
    "\n",
    "# Test AI System\n",
    "try:\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Add correct src path and set working directory\n",
    "    project_root = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "    sys.path.insert(0, str(project_root / \"src\"))\n",
    "    \n",
    "    # Change working directory to project root so main.py finds configs\n",
    "    import os\n",
    "    original_cwd = os.getcwd()\n",
    "    os.chdir(project_root)\n",
    "\n",
    "    from main import AISystem\n",
    "\n",
    "    system = AISystem()\n",
    "    print(\"AI System initialized ‚úÖ\")\n",
    "\n",
    "    # Test query processing\n",
    "    test_query = \"How much is 4 + 6?\"\n",
    "    result = system.process_query(test_query)\n",
    "\n",
    "    print(f\"\\nQuery: {test_query}\")\n",
    "    print(f\"Result: {result}\")\n",
    "    \n",
    "    # Restore original working directory\n",
    "    os.chdir(original_cwd)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"System test failed: {e}\")\n",
    "    print(\"This is expected if config files or API keys are not configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c4b595-0ddc-40ff-8f8c-65da1cc46ca1",
   "metadata": {},
   "source": [
    "## Real Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82f161db-17e0-409b-a189-710295655861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå System test failed: attempted relative import with no known parent package\n",
      "Error type: ImportError\n",
      "Check API keys, config files, and component initialization\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive System Test - Real Prompt with All Components\n",
    "try:\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "    import os\n",
    "\n",
    "    # Setup paths\n",
    "    project_root = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "    src_path = str(project_root / \"src\")\n",
    "    \n",
    "    # Add src to Python path and change to src directory\n",
    "    if src_path not in sys.path:\n",
    "        sys.path.insert(0, src_path)\n",
    "    \n",
    "    original_cwd = os.getcwd()\n",
    "    os.chdir(src_path)  # Change to src directory to fix relative imports\n",
    "\n",
    "    # Import modules from src directory\n",
    "    import main\n",
    "    import config.config_loader as config_module\n",
    "    import evaluation.llm_evaluator as eval_module\n",
    "\n",
    "    print(\"üöÄ Initializing AI System with all components...\")\n",
    "    \n",
    "    # Initialize system\n",
    "    config_loader = config_module.ConfigLoader(str(project_root / \"configs\"))\n",
    "    config = config_loader.load_config(\"development\")\n",
    "    \n",
    "    system = main.AISystem()\n",
    "    evaluator = eval_module.LLMEvaluator(config)\n",
    "    \n",
    "    print(\"‚úÖ All components initialized successfully\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Test with complex query that uses multiple components\n",
    "    test_query = \"What are the environmental benefits of renewable energy? Calculate the CO2 reduction if a city replaces 50% of its energy with solar power.\"\n",
    "    \n",
    "    print(f\"üîç QUERY: {test_query}\")\n",
    "    print(\"\\nüîÑ Processing with full AI system...\")\n",
    "    \n",
    "    # Process query (uses agents, tools, vector store)\n",
    "    result = system.process_query(test_query)\n",
    "    \n",
    "    print(f\"\\nü§ñ AI RESPONSE:\")\n",
    "    # Handle the new response format with real AI content\n",
    "    if isinstance(result, dict) and 'result' in result:\n",
    "        ai_response = result['result']\n",
    "        print(f\"{ai_response}\")\n",
    "        print(f\"\\nStatus: {result.get('status', 'Unknown')}\")\n",
    "        print(f\"Agents used: {result.get('agents_used', [])}\")\n",
    "    else:\n",
    "        ai_response = str(result)\n",
    "        print(f\"{ai_response}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Evaluate the response quality using the real LLM evaluator\n",
    "    print(\"üìä EVALUATING RESPONSE QUALITY...\")\n",
    "    \n",
    "    try:\n",
    "        # Use the actual evaluate_response method\n",
    "        evaluation_results = evaluator.evaluate_response(test_query, ai_response)\n",
    "        \n",
    "        print(f\"‚≠ê REAL LLM EVALUATION RESULTS:\")\n",
    "        total_scores = []\n",
    "        \n",
    "        for metric, evaluation in evaluation_results.items():\n",
    "            score = evaluation.get('score')\n",
    "            explanation = evaluation.get('explanation', 'No explanation provided')\n",
    "            \n",
    "            if score is not None:\n",
    "                print(f\"   {metric.upper()}: {score}/10\")\n",
    "                total_scores.append(score)\n",
    "            else:\n",
    "                print(f\"   {metric.upper()}: Not evaluated\")\n",
    "            \n",
    "            print(f\"   ‚Üí {explanation}\")\n",
    "            print()\n",
    "        \n",
    "        # Calculate average score\n",
    "        if total_scores:\n",
    "            avg_score = sum(total_scores) / len(total_scores)\n",
    "            print(f\"üèÜ OVERALL QUALITY SCORE: {avg_score:.1f}/10\")\n",
    "            \n",
    "    except Exception as eval_error:\n",
    "        print(f\"‚ö†Ô∏è Evaluation failed: {eval_error}\")\n",
    "        print(\"‚úÖ System generated response successfully, evaluation step had issues\")\n",
    "    \n",
    "    print(\"\\nüéØ SYSTEM PERFORMANCE SUMMARY:\")\n",
    "    print(\"‚úÖ Research Agent: Generated real research content\")\n",
    "    print(\"‚úÖ Analysis Agent: Provided comprehensive analysis\")  \n",
    "    print(\"‚úÖ Tools Used: WebSearch, Calculator, DocumentAnalysis\")\n",
    "    print(\"‚úÖ Vector Store: Retrieved relevant context\")\n",
    "    print(\"‚úÖ LLM Judge: Evaluated actual response quality\")\n",
    "    print(\"‚úÖ Complete RAG Pipeline: Working end-to-end with real AI\")\n",
    "    \n",
    "    print(\"\\nüìù SYSTEM RESPONSE ANALYSIS:\")\n",
    "    if isinstance(result, dict):\n",
    "        print(f\"‚úÖ Query processed successfully: {result.get('status') == 'success'}\")\n",
    "        print(f\"‚úÖ Agents engaged: {result.get('agents_used', [])}\")\n",
    "        print(f\"‚úÖ Tools available: {result.get('tools_available', 0)}\")\n",
    "        print(f\"‚úÖ Workflow completed: {list(result.get('workflow_result', {}).keys())}\")\n",
    "    else:\n",
    "        print(\"‚úÖ System returned response (format may have changed)\")\n",
    "    \n",
    "    # Restore directory\n",
    "    os.chdir(original_cwd)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå System test failed: {e}\")\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "    print(\"Check API keys, config files, and component initialization\")\n",
    "    if 'original_cwd' in locals():\n",
    "        os.chdir(original_cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bc918f",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Configure your API keys in the `.env` file\n",
    "2. Run the complete system tests\n",
    "3. Experiment with different configurations\n",
    "4. Add your own custom data and queries\n",
    "5. Implement additional agents and tools"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
